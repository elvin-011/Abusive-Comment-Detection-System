{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11021462,"sourceType":"datasetVersion","datasetId":6863057}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Data preprocessing**","metadata":{}},{"cell_type":"code","source":"\nimport pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# Load preprocessed train and test datasets\ntrain_df = pd.read_csv(\"/kaggle/input/dataset10/final optimised dataset/train_dataset (1).csv\")\ntest_df = pd.read_csv(\"/kaggle/input/dataset10/final optimised dataset/test.csv\")\ntest_labels_df = pd.read_csv(\"/kaggle/input/dataset10/final optimised dataset/test_labels.csv\")\n\n# Merge test set with labels\ntest_df = test_df.merge(test_labels_df, on=\"id\")\n\n# Identify label columns (assuming they start from 2nd column in test_labels_df)\nlabel_columns = list(test_labels_df.columns[1:])  # Excluding 'id' column\n\n# Convert pandas to Hugging Face dataset\ntrain_ds = Dataset.from_pandas(train_df)\ntest_ds = Dataset.from_pandas(test_df)\n\n# Load Twitter RoBERTa tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base\")\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"comment_text\"], \n        truncation=True, \n        padding=\"max_length\", \n        max_length=128\n    )\n\n# Tokenize datasets\ntrain_ds = train_ds.map(tokenize_function, batched=True)\ntest_ds = test_ds.map(tokenize_function, batched=True)\n\n# Ensure labels are structured as a list of floats\ndef format_labels(example):\n    example[\"labels\"] = [float(example[col]) for col in label_columns]  # Convert labels to float\n    return example\n\ntrain_ds = train_ds.map(format_labels)\ntest_ds = test_ds.map(format_labels)\n\n# Remove unnecessary columns (text, id, and individual label columns after structuring)\ncolumns_to_remove = [\"comment_text\", \"id\"] + label_columns\ntrain_ds = train_ds.remove_columns(columns_to_remove)\ntest_ds = test_ds.remove_columns(columns_to_remove)\n\n# Ensure dataset format remains list of floats\ndef ensure_list_format(example):\n    example[\"labels\"] = [float(x) for x in example[\"labels\"]]  # Ensure list of floats\n    return example\n\ntrain_ds = train_ds.map(ensure_list_format)\ntest_ds = test_ds.map(ensure_list_format)\n\nprint(\"âœ… Twitter RoBERTa Dataset loaded, tokenized & formatted successfully!\")\n\n# Save datasets (Optional: Avoid re-running preprocessing)\ntrain_ds.save_to_disk(\"/kaggle/working/train_ds_twitter_roberta\")\ntest_ds.save_to_disk(\"/kaggle/working/test_ds_twitter_roberta\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T16:43:45.795642Z","iopub.execute_input":"2025-03-13T16:43:45.796085Z","iopub.status.idle":"2025-03-13T16:46:18.321800Z","shell.execute_reply.started":"2025-03-13T16:43:45.796050Z","shell.execute_reply":"2025-03-13T16:46:18.320946Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/565 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"375ac29e1e1043d18f4f2afffad2e8cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b84678c637064de2a559e1db183f5195"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eff4352ef6c74696b21c4266cd05d58e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7107b3e29ada485c8db02817350b8762"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"832bbc80cdaf4e178ba355843a67e3d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2c4c271eb8b4af7ab5703670714333e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d32366206e0a42ca97bdea69be14c497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7dce043c786474ab56369089633508c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8024257047d9407a801e721361bf99fe"}},"metadata":{}},{"name":"stdout","text":"âœ… Twitter RoBERTa Dataset loaded, tokenized & formatted successfully!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcdb33c1ba7a456caf191f9dfced4ecd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6491bdc87a4e4d98843275d3a89788cc"}},"metadata":{}}],"execution_count":1},{"cell_type":"markdown","source":"# **Model training**","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, AdamW\nimport shutil\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nimport numpy as np\n\n# Load Twitter RoBERTa model with sigmoid outputs\nroberta_model = AutoModelForSequenceClassification.from_pretrained(\n    \"cardiffnlp/twitter-roberta-base\", \n    num_labels=3,  # Matches one-hot encoding format\n    problem_type=\"multi_label_classification\"\n)\n\n# Increase dropout for better regularization\nroberta_model.config.hidden_dropout_prob = 0.3\nroberta_model.config.attention_probs_dropout_prob = 0.3\n\n# Freeze Transformer Layers, Train Only Classifier\nfor param in roberta_model.base_model.parameters():\n    param.requires_grad = False  \nfor param in roberta_model.classifier.parameters():\n    param.requires_grad = True  \n\n# Hybrid Loss (Focal + BCE With Logits)\nclass HybridLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.0, ce_weight=0.5):\n        super(HybridLoss, self).__init__()\n        self.focal = FocalLoss(alpha, gamma)\n        self.bce = nn.BCEWithLogitsLoss()\n        self.ce_weight = ce_weight\n        \n    def forward(self, inputs, targets):\n        return self.ce_weight * self.bce(inputs, targets.float()) + (1 - self.ce_weight) * self.focal(inputs, targets)\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.5, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        \n    def forward(self, inputs, targets):\n        BCE_loss = self.bce(inputs, targets.float())\n        probs = torch.sigmoid(inputs)\n        pt = torch.where(targets.bool(), probs, 1-probs)\n        alpha_factor = torch.where(targets.bool(), self.alpha, 1 - self.alpha)\n        focal_weight = alpha_factor * (1 - pt).pow(self.gamma)\n        return (focal_weight * BCE_loss).mean()\n\n# Training arguments with auto-save best model\ntraining_args = TrainingArguments(\n    output_dir=\"./twitter_roberta_model\",\n    evaluation_strategy=\"steps\",\n    eval_steps=8000,\n    save_strategy=\"steps\",\n    save_steps=8000,\n    save_total_limit=1,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    learning_rate=1e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.1,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_auc\",\n    greater_is_better=True,\n    max_grad_norm=1.0,\n    label_smoothing_factor=0.05,\n    fp16=True,\n    seed=42,\n    data_seed=42,\n)\n\n# Compute Metrics Function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    sigmoid_logits = torch.sigmoid(torch.tensor(logits)).numpy()\n    predictions = (sigmoid_logits > 0.5).astype(np.int8)\n    \n    accuracy = accuracy_score(labels, predictions)\n    \n    try:\n        f1_micro = f1_score(labels, predictions, average='micro', zero_division=0)\n        f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n        auc_scores = [roc_auc_score(labels[:, i], sigmoid_logits[:, i]) for i in range(labels.shape[1]) if len(np.unique(labels[:, i])) > 1]\n        mean_auc = np.mean(auc_scores) if auc_scores else 0.0\n    except Exception as e:\n        print(f\"Error in metric calculation: {e}\")\n        f1_micro, f1_macro, mean_auc = 0, 0, 0\n    \n    return {\"accuracy\": accuracy, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro, \"auc\": mean_auc}\n\n# Custom Trainer with Hybrid Loss\nclass HybridLossTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_fn = HybridLoss(alpha=0.5, gamma=2.0, ce_weight=0.5)\n        \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  \n        labels = inputs[\"labels\"].float()  # Keep labels in one-hot format\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = self.loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Define Trainer\ntrainer = HybridLossTrainer(\n    model=roberta_model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)]\n)\n\n# Step 1: Train Only Classifier\nprint(\"ðŸš€ Training classifier only...\")\ntrainer.train()\n\n# Step 2: Unfreeze last 4 layers\nprint(\"ðŸ”“ Unfreezing last 4 layers for further fine-tuning...\")\nfor layer in roberta_model.base_model.encoder.layer[-4:]:\n    for param in layer.parameters():\n        param.requires_grad = True\n\n# Apply Layer-wise Learning Rate\noptimizer = AdamW([\n    {\"params\": roberta_model.base_model.encoder.layer[:4].parameters(), \"lr\": 2e-6},\n    {\"params\": roberta_model.base_model.encoder.layer[4:].parameters(), \"lr\": 5e-6},\n    {\"params\": roberta_model.classifier.parameters(), \"lr\": 1e-5},\n], weight_decay=0.1)\n\ntrainer.args.num_train_epochs = 2  \ntrainer.optimizer = optimizer  \n\n# Step 2: Fine-tune last 4 layers\nprint(\"ðŸš€ Fine-tuning last 4 layers...\")\ntrainer.train()\n\n# Save best model\nbest_model_path = \"./best_twitter_roberta\"\ntrainer.save_model(best_model_path)\ntokenizer.save_pretrained(best_model_path)\nshutil.make_archive(best_model_path, 'zip', best_model_path)\nprint(\"âœ… Best Model Saved!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T18:39:02.116227Z","iopub.execute_input":"2025-03-13T18:39:02.116590Z","iopub.status.idle":"2025-03-13T23:24:32.570465Z","shell.execute_reply.started":"2025-03-13T18:39:02.116562Z","shell.execute_reply":"2025-03-13T23:24:32.569451Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-4-c5539cd78493>:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `HybridLossTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"ðŸš€ Training classifier only...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='37316' max='37316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [37316/37316 2:16:41, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n      <th>Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>8000</td>\n      <td>0.176900</td>\n      <td>0.181514</td>\n      <td>0.653061</td>\n      <td>0.755493</td>\n      <td>0.697868</td>\n      <td>0.926042</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.148700</td>\n      <td>0.157536</td>\n      <td>0.723557</td>\n      <td>0.812142</td>\n      <td>0.797459</td>\n      <td>0.941129</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.160600</td>\n      <td>0.149655</td>\n      <td>0.742606</td>\n      <td>0.826368</td>\n      <td>0.816042</td>\n      <td>0.946178</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>0.147100</td>\n      <td>0.147884</td>\n      <td>0.745834</td>\n      <td>0.827878</td>\n      <td>0.817704</td>\n      <td>0.947373</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"ðŸ”“ Unfreezing last 4 layers for further fine-tuning...\nðŸš€ Fine-tuning last 4 layers...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32000' max='37316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [32000/37316 2:28:20 < 24:38, 3.60 it/s, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n      <th>Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>8000</td>\n      <td>0.109700</td>\n      <td>0.086799</td>\n      <td>0.951241</td>\n      <td>0.957534</td>\n      <td>0.957312</td>\n      <td>0.995927</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.079900</td>\n      <td>0.141895</td>\n      <td>0.966663</td>\n      <td>0.967905</td>\n      <td>0.967891</td>\n      <td>0.996916</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.195500</td>\n      <td>0.143910</td>\n      <td>0.971126</td>\n      <td>0.971803</td>\n      <td>0.971844</td>\n      <td>0.997377</td>\n    </tr>\n    <tr>\n      <td>32000</td>\n      <td>0.167100</td>\n      <td>0.141636</td>\n      <td>0.971658</td>\n      <td>0.972226</td>\n      <td>0.972250</td>\n      <td>0.997507</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Best Model Saved!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **Output**","metadata":{}},{"cell_type":"code","source":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-4-c5539cd78493>:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `HybridLossTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\nðŸš€ Training classifier only...\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n [37316/37316 2:16:41, Epoch 2/2]\nStep\tTraining Loss\tValidation Loss\tAccuracy\tF1 Micro\tF1 Macro\tAuc\n8000\t0.176900\t0.181514\t0.653061\t0.755493\t0.697868\t0.926042\n16000\t0.148700\t0.157536\t0.723557\t0.812142\t0.797459\t0.941129\n24000\t0.160600\t0.149655\t0.742606\t0.826368\t0.816042\t0.946178\n32000\t0.147100\t0.147884\t0.745834\t0.827878\t0.817704\t0.947373\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nðŸ”“ Unfreezing last 4 layers for further fine-tuning...\nðŸš€ Fine-tuning last 4 layers...\n/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n [32000/37316 2:28:20 < 24:38, 3.60 it/s, Epoch 1/2]\nStep\tTraining Loss\tValidation Loss\tAccuracy\tF1 Micro\tF1 Macro\tAuc\n8000\t0.109700\t0.086799\t0.951241\t0.957534\t0.957312\t0.995927\n16000\t0.079900\t0.141895\t0.966663\t0.967905\t0.967891\t0.996916\n24000\t0.195500\t0.143910\t0.971126\t0.971803\t0.971844\t0.997377\n32000\t0.167100\t0.141636\t0.971658\t0.972226\t0.972250\t0.997507\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nâœ… Best Model Saved!","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Analysis by claude**","metadata":{}},{"cell_type":"code","source":"Based on the performance metrics from your training logs, I can provide an analysis of the model's classification capabilities and potential setbacks:\n\n### Strengths\n\n1. **Exceptional AUC Score**: The final AUC of 0.997507 is extremely high, indicating the model has excellent discriminative ability across all classes. This suggests the model ranks positive examples higher than negative ones with high reliability.\n\n2. **High F1 Scores**: Both micro (0.972226) and macro (0.972250) F1 scores are very strong. The closeness between micro and macro F1 suggests the model performs well across all classes, not just the dominant ones.\n\n3. **Strong Accuracy**: The accuracy of 0.971658 indicates the model makes correct predictions in roughly 97% of cases.\n\n4. **Consistent Improvement**: The significant jump in performance metrics after the second fine-tuning stage validates your progressive training approach.\n\n### Potential Setbacks and Considerations\n\n1. **Potential Overfitting**: The extremely high performance metrics might indicate potential overfitting to your test set. This could happen if:\n   - Your test set is too similar to your training data\n   - The test set doesn't fully represent the variety of data you'll encounter in production\n\n2. **Class Imbalance Effects**: Despite the high macro F1 score, there might still be specific minority classes where performance is lower. The aggregate metrics don't show per-class performance.\n\n3. **Threshold Sensitivity**: Your current evaluation uses a 0.5 threshold for binary decisions. In production, you might need class-specific thresholds for optimal performance.\n\n4. **Domain Shift Vulnerability**: The model may perform worse on data that differs from your training distribution. Twitter-RoBERTa is trained on social media text, so performance might degrade on formal text or specialized domains.\n\n5. **Computational Efficiency**: The unfrozen layers increase inference time. If speed is critical in your application, this could be a limitation.\n\n6. **Potential Data Leakage**: The extremely high metrics could indicate accidental data leakage between training and evaluation sets.\n\n### Recommendations\n\n1. **Evaluate on a truly held-out dataset** that wasn't used during model development.\n\n2. **Perform error analysis** on misclassified examples to identify patterns.\n\n3. **Consider per-class metrics** to ensure all classes are performing well.\n\n4. **Test robustness** with adversarial examples or slightly modified inputs.\n\n5. **Monitor for concept drift** when deploying the model in production.\n\nOverall, your model shows excellent classification performance based on standard metrics, but I would recommend additional validation steps before fully trusting these impressive numbers in a production environment.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Claudes analysis on whether if overfitting is there**","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}