{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10997352,"sourceType":"datasetVersion","datasetId":6845810},{"sourceId":11039363,"sourceType":"datasetVersion","datasetId":6876153}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# Load preprocessed train and test datasets\ntrain_df = pd.read_csv(\"/kaggle/input/opt-dataset/final optimised dataset/train_dataset (1).csv\")\ntest_df = pd.read_csv(\"/kaggle/input/opt-dataset/final optimised dataset/test.csv\")\ntest_labels_df = pd.read_csv(\"/kaggle/input/opt-dataset/final optimised dataset/test_labels.csv\")\n\n# Merge test set with labels\ntest_df = test_df.merge(test_labels_df, on=\"id\")\n\n# Identify label columns (assuming they start from 2nd column in test_labels_df)\nlabel_columns = list(test_labels_df.columns[1:])  # Excluding 'id' column\n\n# Convert pandas to Hugging Face dataset\ntrain_ds = Dataset.from_pandas(train_df)\ntest_ds = Dataset.from_pandas(test_df)\n\n# Load Deberta tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"GroNLP/hateBERT\")\n\n# First, check if the column exists and what type of data it contains\nprint(test_ds.column_names)  # Check if \"comment_text\" exists\nprint(type(test_ds[\"comment_text\"][0]) if \"comment_text\" in test_ds.column_names else \"Column not found\")\n\n# Modified tokenization function with error handling\ndef tokenize_function(examples):\n    # Filter out None/NaN values and ensure all inputs are strings\n    texts = []\n    for text in examples[\"comment_text\"]:\n        if text is None:\n            texts.append(\"\")  # Replace None with empty string\n        elif not isinstance(text, str):\n            texts.append(str(text))  # Convert non-strings to strings\n        else:\n            texts.append(text)\n    \n    return tokenizer(\n        texts,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"np\"  # Use numpy arrays instead of PyTorch tensors for now\n    )\n\n# Try processing in smaller batches\ntrain_ds = train_ds.map(tokenize_function, batched=True, batch_size=32)\ntest_ds = test_ds.map(tokenize_function, batched=True, batch_size=32)\n\n# Ensure labels are structured as a list of floats\ndef format_labels(example):\n    example[\"labels\"] = [float(example[col]) for col in label_columns]  # Convert labels to float\n    return example\n\ntrain_ds = train_ds.map(format_labels)\ntest_ds = test_ds.map(format_labels)\n\n# Remove unnecessary columns (text, id, and individual label columns after structuring)\ncolumns_to_remove = [\"comment_text\", \"id\"] + label_columns\ntrain_ds = train_ds.remove_columns(columns_to_remove)\ntest_ds = test_ds.remove_columns(columns_to_remove)\n\n# Ensure dataset format remains list of floats\ndef ensure_list_format(example):\n    example[\"labels\"] = [float(x) for x in example[\"labels\"]]  # Ensure list of floats\n    return example\n\ntrain_ds = train_ds.map(ensure_list_format)\ntest_ds = test_ds.map(ensure_list_format)\n\nprint(\"âœ… Hatebert Dataset loaded, tokenized & formatted successfully!\")\n\n# Save datasets (Optional: Avoid re-running preprocessing)\ntrain_ds.save_to_disk(\"/kaggle/working/train_ds_hatebert\")\ntest_ds.save_to_disk(\"/kaggle/working/test_ds_hatebert\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T10:57:31.430421Z","iopub.execute_input":"2025-03-15T10:57:31.430751Z","iopub.status.idle":"2025-03-15T11:00:32.206642Z","shell.execute_reply.started":"2025-03-15T10:57:31.430724Z","shell.execute_reply":"2025-03-15T11:00:32.205915Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/151 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25cf2de8319247debdaed86a9913455f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.24k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5769c6c4878497b815610aca17029a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ca0b9c19db145d79a979dc4b715d439"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ea102e92f724096a48ce5a510992c1d"}},"metadata":{}},{"name":"stdout","text":"['id', 'comment_text', 'severely_toxic', 'moderately_toxic', 'non_toxic']\n<class 'str'>\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3afd205f5a43bb9d1f3c9fe3732599"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e10097cbeca414880aaf5e3effcd753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6560c66c361a4532a0cc0c0797e68586"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b8404db77214412ada483a9d13f8772"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fa37fc3cbe542a496b27ec65b06a1b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19eb95d6acca40e7ae0ae93e9d7f888b"}},"metadata":{}},{"name":"stdout","text":"âœ… Hatebert Dataset loaded, tokenized & formatted successfully!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"788629f6e4c04ad7b232577553635092"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db3759cb757d4e1c9dfa7577ed98f284"}},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, AdamW\nimport shutil\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nimport numpy as np\n\n# Load HateBERT model with sigmoid outputs for multi-label classification\nhatebert_model = AutoModelForSequenceClassification.from_pretrained(\n    \"GroNLP/hateBERT\", \n    num_labels=3,  # Matches one-hot encoding format\n    problem_type=\"multi_label_classification\"\n)\n\n# Increase dropout for better regularization\nhatebert_model.config.hidden_dropout_prob = 0.3\nhatebert_model.config.attention_probs_dropout_prob = 0.3\n\n# Freeze Transformer Layers, Train Only Classifier\nfor param in hatebert_model.bert.parameters():\n    param.requires_grad = False  \nfor param in hatebert_model.classifier.parameters():\n    param.requires_grad = True  \n\n# Hybrid Loss (Focal + BCE With Logits)\nclass HybridLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.0, ce_weight=0.5):\n        super(HybridLoss, self).__init__()\n        self.focal = FocalLoss(alpha, gamma)\n        self.bce = nn.BCEWithLogitsLoss()\n        self.ce_weight = ce_weight\n        \n    def forward(self, inputs, targets):\n        return self.ce_weight * self.bce(inputs, targets.float()) + (1 - self.ce_weight) * self.focal(inputs, targets)\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.5, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        \n    def forward(self, inputs, targets):\n        BCE_loss = self.bce(inputs, targets.float())\n        probs = torch.sigmoid(inputs)\n        pt = torch.where(targets.bool(), probs, 1-probs)\n        alpha_factor = torch.where(targets.bool(), self.alpha, 1 - self.alpha)\n        focal_weight = alpha_factor * (1 - pt).pow(self.gamma)\n        return (focal_weight * BCE_loss).mean()\n\n# Training arguments with auto-save best model\ntraining_args = TrainingArguments(\n    output_dir=\"./hatebert_model\",\n    evaluation_strategy=\"steps\",\n    eval_steps=24876,\n    save_strategy=\"steps\",\n    save_steps=24876,\n    save_total_limit=1,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    learning_rate=1e-5,\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.1,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_auc\",\n    greater_is_better=True,\n    max_grad_norm=1.0,\n    label_smoothing_factor=0.05,\n    fp16=True,\n    seed=42,\n    data_seed=42,\n)\n\n# Compute Metrics Function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    sigmoid_logits = torch.sigmoid(torch.tensor(logits)).numpy()\n    predictions = (sigmoid_logits > 0.5).astype(np.int8)\n    \n    accuracy = accuracy_score(labels, predictions)\n    \n    try:\n        f1_micro = f1_score(labels, predictions, average='micro', zero_division=0)\n        f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n        auc_scores = [roc_auc_score(labels[:, i], sigmoid_logits[:, i]) for i in range(labels.shape[1]) if len(np.unique(labels[:, i])) > 1]\n        mean_auc = np.mean(auc_scores) if auc_scores else 0.0\n    except Exception as e:\n        print(f\"Error in metric calculation: {e}\")\n        f1_micro, f1_macro, mean_auc = 0, 0, 0\n    \n    return {\"accuracy\": accuracy, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro, \"auc\": mean_auc}\n\n# Custom Trainer with Hybrid Loss\nclass HybridLossTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_fn = HybridLoss(alpha=0.5, gamma=2.0, ce_weight=0.5)\n        \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  \n        labels = inputs[\"labels\"].float()  # Keep labels in one-hot format\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = self.loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Define Trainer\ntrainer = HybridLossTrainer(\n    model=hatebert_model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)]\n)\n\n# Step 1: Train Only Classifier\nprint(\"ðŸš€ Training classifier only...\")\ntrainer.train()\n\n# Step 2: Unfreeze last 4 layers\nprint(\"ðŸ”“ Unfreezing last 4 layers for further fine-tuning...\")\nfor layer in hatebert_model.bert.encoder.layer[-4:]:\n    for param in layer.parameters():\n        param.requires_grad = True\n\n# Apply Layer-wise Learning Rate\noptimizer = AdamW([\n    {\"params\": hatebert_model.bert.encoder.layer[:8].parameters(), \"lr\": 2e-6},\n    {\"params\": hatebert_model.bert.encoder.layer[8:].parameters(), \"lr\": 5e-6},\n    {\"params\": hatebert_model.classifier.parameters(), \"lr\": 1e-5},\n], weight_decay=0.1)\n\ntrainer.args.num_train_epochs = 2  \ntrainer.optimizer = optimizer  \n\n# Step 2: Fine-tune last 4 layers\nprint(\"ðŸš€ Fine-tuning last 4 layers...\")\ntrainer.train()\n\n# Save best model\nbest_model_path = \"./best_hatebert\"\ntrainer.save_model(best_model_path)\ntokenizer.save_pretrained(best_model_path)\nshutil.make_archive(best_model_path, 'zip', best_model_path)\nprint(\"âœ… Best Model Saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T11:03:31.618687Z","iopub.execute_input":"2025-03-15T11:03:31.619052Z","iopub.status.idle":"2025-03-15T14:06:16.905372Z","shell.execute_reply.started":"2025-03-15T11:03:31.618998Z","shell.execute_reply":"2025-03-15T14:06:16.904575Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at GroNLP/hateBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-5-4aa5ea5a290e>:101: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `HybridLossTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"ðŸš€ Training classifier only...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='74630' max='74630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [74630/74630 1:15:16, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n      <th>Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>24876</td>\n      <td>0.274000</td>\n      <td>0.264844</td>\n      <td>0.462208</td>\n      <td>0.595678</td>\n      <td>0.508486</td>\n      <td>0.880075</td>\n    </tr>\n    <tr>\n      <td>49752</td>\n      <td>0.239600</td>\n      <td>0.241750</td>\n      <td>0.529179</td>\n      <td>0.649256</td>\n      <td>0.569643</td>\n      <td>0.895247</td>\n    </tr>\n    <tr>\n      <td>74628</td>\n      <td>0.233000</td>\n      <td>0.238685</td>\n      <td>0.540145</td>\n      <td>0.657270</td>\n      <td>0.577680</td>\n      <td>0.896979</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"ðŸ”“ Unfreezing last 4 layers for further fine-tuning...\nðŸš€ Fine-tuning last 4 layers...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='74630' max='74630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [74630/74630 1:47:04, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n      <th>Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>24876</td>\n      <td>0.110200</td>\n      <td>0.055882</td>\n      <td>0.959988</td>\n      <td>0.961953</td>\n      <td>0.961889</td>\n      <td>0.996332</td>\n    </tr>\n    <tr>\n      <td>49752</td>\n      <td>0.066700</td>\n      <td>0.045776</td>\n      <td>0.967257</td>\n      <td>0.968819</td>\n      <td>0.968801</td>\n      <td>0.997309</td>\n    </tr>\n    <tr>\n      <td>74628</td>\n      <td>0.040900</td>\n      <td>0.043244</td>\n      <td>0.969055</td>\n      <td>0.970655</td>\n      <td>0.970684</td>\n      <td>0.997459</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"âœ… Best Model Saved!\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}