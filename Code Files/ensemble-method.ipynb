{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-03T18:11:09.325737Z",
     "iopub.status.busy": "2025-04-03T18:11:09.325447Z",
     "iopub.status.idle": "2025-04-03T18:11:12.480556Z",
     "shell.execute_reply": "2025-04-03T18:11:12.479695Z",
     "shell.execute_reply.started": "2025-04-03T18:11:09.325714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mismatch in predictions shape for electra. Expected (156638, 3), got (63751, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m preds = np.load(save_path)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m preds.shape != (\u001b[38;5;28mlen\u001b[39m(comments), \u001b[32m3\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMismatch in predictions shape for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(comments),\u001b[38;5;250m \u001b[39m\u001b[32m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m ensemble_preds += preds / \u001b[38;5;28mlen\u001b[39m(MODEL_NAMES)  \u001b[38;5;66;03m# Uniform weighting\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded predictions from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mValueError\u001b[39m: Mismatch in predictions shape for electra. Expected (156638, 3), got (63751, 3)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, auc\n",
    "import os\n",
    "\n",
    "# Model Names\n",
    "MODEL_NAMES = [\"electra\", \"deberta\", \"roberta\", \"hatebert\"]\n",
    "\n",
    "# Load Test Dataset\n",
    "df = pd.read_csv(\"D:/Project Phase 1/Toxic-Comment-Classification-Challenge-master/shuffled_dataset.csv\")  # Update with actual path\n",
    "\n",
    "# Map text labels to numerical values\n",
    "label_mapping = {\n",
    "    \"non_toxic\": 2,\n",
    "    \"moderately_toxic\": 1,\n",
    "    \"severely_toxic\": 0\n",
    "}\n",
    "df[\"label\"] = df[\"label\"].map(label_mapping)\n",
    "\n",
    "comments = df[\"comment_text\"].tolist()\n",
    "true_labels = df[\"label\"].values  # Numerical labels (0, 1, 2)\n",
    "\n",
    "# Directory containing saved predictions\n",
    "SAVE_DIR = \"D:/Project Phase 1/predictions\"\n",
    "\n",
    "# Load predictions from saved .npy files\n",
    "ensemble_preds = np.zeros((len(comments), 3))  # Assuming 3 classes: 0, 1, 2\n",
    "\n",
    "for model_name in MODEL_NAMES:\n",
    "    save_path = os.path.join(SAVE_DIR, f\"{model_name}_predictions.npy\")\n",
    "\n",
    "    if os.path.exists(save_path):\n",
    "        preds = np.load(save_path)\n",
    "        if preds.shape != (len(comments), 3):\n",
    "            raise ValueError(f\"Mismatch in predictions shape for {model_name}. Expected {(len(comments), 3)}, got {preds.shape}\")\n",
    "        \n",
    "        ensemble_preds += preds / len(MODEL_NAMES)  # Uniform weighting\n",
    "        print(f\"Loaded predictions from {save_path}\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Prediction file not found: {save_path}\")\n",
    "\n",
    "# Save ensemble predictions using pandas\n",
    "df_preds = pd.DataFrame(ensemble_preds, columns=[\"Non-Toxic\", \"Moderately Toxic\", \"Severely Toxic\"])\n",
    "df_preds.to_csv(\"ensemble_predictions.csv\", index=False)\n",
    "\n",
    "# Final Prediction based on Averaging\n",
    "final_predictions = np.argmax(ensemble_preds, axis=1)\n",
    "\n",
    "# Evaluate Model\n",
    "accuracy = accuracy_score(true_labels, final_predictions)\n",
    "f1 = f1_score(true_labels, final_predictions, average=\"macro\")\n",
    "conf_matrix = confusion_matrix(true_labels, final_predictions)\n",
    "class_report = classification_report(true_labels, final_predictions)\n",
    "auc_score = roc_auc_score(true_labels, ensemble_preds, multi_class=\"ovr\")\n",
    "\n",
    "# Print Results\n",
    "print(f\"Ensemble Model Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Ensemble Model F1-Score: {f1:.4f}\")\n",
    "print(\"Confusion Matrix:/n\", conf_matrix)\n",
    "print(\"Classification Report:/n\", class_report)\n",
    "print(f\"Ensemble Model AUC: {auc_score:.4f}\")\n",
    "\n",
    "# Function to Save Evaluation Report\n",
    "def save_evaluation_report(accuracy, f1, conf_matrix, class_report, auc_score, true_labels, ensemble_preds, label_mapping):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))  # 2x2 grid\n",
    "\n",
    "    # Plot Confusion Matrix\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys(), ax=axes[0, 0])\n",
    "    axes[0, 0].set_xlabel(\"Predicted Label\")\n",
    "    axes[0, 0].set_ylabel(\"True Label\")\n",
    "    axes[0, 0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "    # Plot ROC Curves\n",
    "    for i, label in enumerate(label_mapping.keys()):\n",
    "        fpr, tpr, _ = roc_curve(true_labels == i, ensemble_preds[:, i])\n",
    "        axes[0, 1].plot(fpr, tpr, label=f\"Class {label} (AUC: {auc(fpr, tpr):.4f})\")\n",
    "\n",
    "    axes[0, 1].plot([0, 1], [0, 1], \"k--\")\n",
    "    axes[0, 1].set_xlabel(\"False Positive Rate\")\n",
    "    axes[0, 1].set_ylabel(\"True Positive Rate\")\n",
    "    axes[0, 1].set_title(\"ROC Curves for Each Class\")\n",
    "    axes[0, 1].legend()\n",
    "\n",
    "    # Accuracy, F1, AUC Score\n",
    "    metrics_text = f\"\"\"\n",
    "    Accuracy: {accuracy:.4f}\n",
    "    F1 Score: {f1:.4f}\n",
    "    AUC Score: {auc_score:.4f}\n",
    "    \"\"\"\n",
    "    axes[1, 0].text(0.5, 0.5, metrics_text, fontsize=12, ha=\"center\", va=\"center\", bbox=dict(boxstyle=\"round\", facecolor=\"lightgray\", alpha=0.5))\n",
    "    axes[1, 0].set_axis_off()\n",
    "    axes[1, 0].set_title(\"Evaluation Metrics\")\n",
    "\n",
    "    # Classification Report\n",
    "    axes[1, 1].text(0.5, 0.5, class_report, fontsize=10, ha=\"center\", va=\"center\", bbox=dict(boxstyle=\"round\", facecolor=\"lightgray\", alpha=0.5))\n",
    "    axes[1, 1].set_axis_off()\n",
    "    axes[1, 1].set_title(\"Classification Report\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save Report\n",
    "    plt.savefig(\"evaluation_report.png\", dpi=300)\n",
    "    print(\"Evaluation report saved as 'evaluation_report.png'\")\n",
    "\n",
    "# Call function to save evaluation report\n",
    "save_evaluation_report(accuracy, f1, conf_matrix, class_report, auc_score, true_labels, ensemble_preds, label_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***OUTPUT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:10:39.796759Z",
     "iopub.status.busy": "2025-04-03T18:10:39.796273Z",
     "iopub.status.idle": "2025-04-03T18:10:39.804011Z",
     "shell.execute_reply": "2025-04-03T18:10:39.802732Z",
     "shell.execute_reply.started": "2025-04-03T18:10:39.796725Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m17\u001b[0m\n\u001b[0;31m    0       1.00      0.98      0.99     42561\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "Loading saved predictions for electra\n",
    "Loading saved predictions for deberta\n",
    "Processing roberta:   0%|          | 0/127936 [00:00<?, ?comments/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
    "Processing roberta: 100%|██████████| 127936/127936 [17:39<00:00, 120.69comments/s]\n",
    "Saved predictions for roberta at model_predictions/roberta_predictions.npy\n",
    "Processing hatebert: 100%|██████████| 127936/127936 [17:29<00:00, 121.86comments/s]\n",
    "Saved predictions for hatebert at model_predictions/hatebert_predictions.npy\n",
    "Ensemble Model Accuracy: 0.9767\n",
    "Ensemble Model F1-Score: 0.9767\n",
    "Confusion Matrix:\n",
    " [[41683   839    39]\n",
    " [  180 41200  1164]\n",
    " [    3   757 42071]]\n",
    "Classification Report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.98      0.99     42561\n",
    "           1       0.96      0.97      0.97     42544\n",
    "           2       0.97      0.98      0.98     42831\n",
    "\n",
    "    accuracy                           0.98    127936\n",
    "   macro avg       0.98      0.98      0.98    127936\n",
    "weighted avg       0.98      0.98      0.98    127936\n",
    "\n",
    "Ensemble Model AUC: 0.9981"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***USER INPUT***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-03T18:10:39.804590Z",
     "iopub.status.idle": "2025-04-03T18:10:39.804896Z",
     "shell.execute_reply": "2025-04-03T18:10:39.804755Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n🔥 Abusive Comment Detection System 🔥/nEnter text to classify (type 'exit' to quit):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Prediction: Moderately Toxic\n",
      "🚀 Prediction: Moderately Toxic\n",
      "🚀 Prediction: Non-Toxic\n",
      "🚀 Prediction: Non-Toxic\n",
      "Exiting...👋\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Model Paths (Update these paths accordingly)\n",
    "MODEL_PATHS = {\n",
    "    \"electra\": \"D:/Project Phase 1/Fine_Tuned Models/best_electra_model (1)\",\n",
    "    \"deberta\": \"D:/Project Phase 1/Fine_Tuned Models/best_deberta\",\n",
    "    \"roberta\": \"D:/Project Phase 1/Fine_Tuned Models/updated_twitter_roberta\",\n",
    "    \"hatebert\": \"D:/Project Phase 1/Fine_Tuned Models/best_hatebert\",\n",
    "}\n",
    "\n",
    "# Load Models and Tokenizers\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    models[name] = AutoModelForSequenceClassification.from_pretrained(path).to(device).eval()\n",
    "\n",
    "# Label Mapping\n",
    "label_mapping = {0: \"Severely Toxic\", 1: \"Moderately Toxic\", 2: \"Non-Toxic\"}\n",
    "\n",
    "def classify_comment(comment):\n",
    "    \"\"\"Classifies a given user input comment using the ensemble model.\"\"\"\n",
    "    ensemble_preds = np.zeros(3)  # Assuming 3 classes: 0, 1, 2\n",
    "    \n",
    "    for model_name in models:\n",
    "        tokenizer = tokenizers[model_name]\n",
    "        model = models[model_name]\n",
    "        \n",
    "        inputs = tokenizer(comment, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs).logits\n",
    "        preds = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "        \n",
    "        ensemble_preds += preds / len(models)  # Uniform weighting\n",
    "    \n",
    "    predicted_label = np.argmax(ensemble_preds)\n",
    "    return label_mapping[predicted_label]\n",
    "\n",
    "# User Input Loop\n",
    "print(\"/n🔥 Abusive Comment Detection System 🔥/nEnter text to classify (type 'exit' to quit):\")\n",
    "while True:\n",
    "    user_input = input(\"/nEnter comment: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Exiting...👋\")\n",
    "        break\n",
    "    result = classify_comment(user_input)\n",
    "    print(f\"🚀 Prediction: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-03T18:11:37.008813Z",
     "iopub.status.busy": "2025-04-03T18:11:37.008509Z",
     "iopub.status.idle": "2025-04-03T18:17:22.739138Z",
     "shell.execute_reply": "2025-04-03T18:17:22.738295Z",
     "shell.execute_reply.started": "2025-04-03T18:11:37.008768Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n🔥 Abusive Comment Detection System 🔥/nEnter text to classify (type 'exit' to quit):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n🔍 Model Probabilities:\n",
      "electra: Non-Toxic: 0.9720, Moderately Toxic: 0.0279, Severely Toxic: 0.0001\n",
      "deberta: Non-Toxic: 0.1366, Moderately Toxic: 0.8629, Severely Toxic: 0.0005\n",
      "roberta: Non-Toxic: 0.7953, Moderately Toxic: 0.2047, Severely Toxic: 0.0000\n",
      "hatebert: Non-Toxic: 0.0006, Moderately Toxic: 0.9993, Severely Toxic: 0.0001\n",
      "/n🎯 Ensemble Probabilities:\n",
      "Non-Toxic: 0.4761, Moderately Toxic: 0.5237, Severely Toxic: 0.0002\n",
      "🚀 Prediction: Moderately Toxic\n",
      "/n🔍 Model Probabilities:\n",
      "electra: Non-Toxic: 0.0027, Moderately Toxic: 0.9848, Severely Toxic: 0.0125\n",
      "deberta: Non-Toxic: 0.0009, Moderately Toxic: 0.9907, Severely Toxic: 0.0085\n",
      "roberta: Non-Toxic: 0.0000, Moderately Toxic: 0.9993, Severely Toxic: 0.0007\n",
      "hatebert: Non-Toxic: 0.0001, Moderately Toxic: 0.9988, Severely Toxic: 0.0011\n",
      "/n🎯 Ensemble Probabilities:\n",
      "Non-Toxic: 0.0009, Moderately Toxic: 0.9934, Severely Toxic: 0.0057\n",
      "🚀 Prediction: Moderately Toxic\n",
      "/n🔍 Model Probabilities:\n",
      "electra: Non-Toxic: 0.0009, Moderately Toxic: 0.9966, Severely Toxic: 0.0024\n",
      "deberta: Non-Toxic: 0.0003, Moderately Toxic: 0.9894, Severely Toxic: 0.0102\n",
      "roberta: Non-Toxic: 0.0003, Moderately Toxic: 0.9932, Severely Toxic: 0.0064\n",
      "hatebert: Non-Toxic: 0.0001, Moderately Toxic: 0.9998, Severely Toxic: 0.0001\n",
      "/n🎯 Ensemble Probabilities:\n",
      "Non-Toxic: 0.0004, Moderately Toxic: 0.9948, Severely Toxic: 0.0048\n",
      "🚀 Prediction: Moderately Toxic\n",
      "/n🔍 Model Probabilities:\n",
      "electra: Non-Toxic: 0.0369, Moderately Toxic: 0.4189, Severely Toxic: 0.5443\n",
      "deberta: Non-Toxic: 0.0041, Moderately Toxic: 0.8929, Severely Toxic: 0.1031\n",
      "roberta: Non-Toxic: 0.0045, Moderately Toxic: 0.8414, Severely Toxic: 0.1541\n",
      "hatebert: Non-Toxic: 0.0544, Moderately Toxic: 0.3389, Severely Toxic: 0.6067\n",
      "/n🎯 Ensemble Probabilities:\n",
      "Non-Toxic: 0.0250, Moderately Toxic: 0.6230, Severely Toxic: 0.3520\n",
      "🚀 Prediction: Moderately Toxic\n",
      "Exiting...👋\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Model Paths (Update these paths accordingly)\n",
    "MODEL_PATHS = {\n",
    "    \"electra\": \"D:/Project Phase 1/Fine_Tuned Models/best_electra_model (1)\",\n",
    "    \"deberta\": \"D:/Project Phase 1/Fine_Tuned Models/best_deberta\",\n",
    "    \"roberta\": \"D:/Project Phase 1/Fine_Tuned Models/updated_twitter_roberta\",\n",
    "    \"hatebert\": \"D:/Project Phase 1/Fine_Tuned Models/best_hatebert\",\n",
    "}\n",
    "\n",
    "# Load Models and Tokenizers\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    models[name] = AutoModelForSequenceClassification.from_pretrained(path).to(device).eval()\n",
    "\n",
    "# Label Mapping\n",
    "label_mapping = {0: \"Severely Toxic\", 1: \"Moderately Toxic\", 2: \"Non-Toxic\"}\n",
    "\n",
    "def classify_comment(comment):\n",
    "    \"\"\"Classifies a given user input comment using the ensemble model and displays individual model probabilities.\"\"\"\n",
    "    ensemble_preds = np.zeros(3)  # Assuming 3 classes: 0, 1, 2\n",
    "    model_probs = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        # if model_name == \"roberta\":\n",
    "        #     # Force RoBERTa to always classify as \"Severely Toxic\"\n",
    "        #     probs = np.array([1.0, 0.0, 0.0])  # 100% Severely Toxic\n",
    "        # else:\n",
    "        tokenizer = tokenizers[model_name]\n",
    "        model = models[model_name]\n",
    "        \n",
    "        inputs = tokenizer(comment, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs).logits\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        model_probs[model_name] = probs\n",
    "        ensemble_preds += probs / len(models)  # Uniform weighting\n",
    "    \n",
    "    predicted_label = np.argmax(ensemble_preds)\n",
    "    \n",
    "    print(\"/n🔍 Model Probabilities:\")\n",
    "    for model_name, probs in model_probs.items():\n",
    "        print(f\"{model_name}: Non-Toxic: {probs[2]:.4f}, Moderately Toxic: {probs[1]:.4f}, Severely Toxic: {probs[0]:.4f}\")\n",
    "    \n",
    "    print(\"/n🎯 Ensemble Probabilities:\")\n",
    "    print(f\"Non-Toxic: {ensemble_preds[2]:.4f}, Moderately Toxic: {ensemble_preds[1]:.4f}, Severely Toxic: {ensemble_preds[0]:.4f}\")\n",
    "    \n",
    "    return label_mapping[predicted_label]\n",
    "\n",
    "# User Input Loop\n",
    "print(\"/n🔥 Abusive Comment Detection System 🔥/nEnter text to classify (type 'exit' to quit):\")\n",
    "while True:\n",
    "    user_input = input(\"/nEnter comment: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Exiting...👋\")\n",
    "        break\n",
    "    result = classify_comment(user_input)\n",
    "    print(f\"🚀 Prediction: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-04-03T18:10:39.806846Z",
     "iopub.status.idle": "2025-04-03T18:10:39.807238Z",
     "shell.execute_reply": "2025-04-03T18:10:39.807072Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n🔥 Abusive Comment Detection System 🔥/nEnter text to classify (type 'exit' to quit):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/n🔍 Model Probabilities:\n",
      "electra: Non-Toxic: 0.0256, Moderately Toxic: 0.5384, Severely Toxic: 0.4360\n",
      "deberta: Non-Toxic: 0.0022, Moderately Toxic: 0.9658, Severely Toxic: 0.0320\n",
      "roberta: Non-Toxic: 0.0031, Moderately Toxic: 0.8440, Severely Toxic: 0.1530\n",
      "hatebert: Non-Toxic: 0.0001, Moderately Toxic: 0.9996, Severely Toxic: 0.0003\n",
      "/n🎯 Ensemble Probabilities:\n",
      "Non-Toxic: 0.0077, Moderately Toxic: 0.8370, Severely Toxic: 0.1553\n",
      "🚀 Prediction: Moderately Toxic\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    101\u001b[39m result, _ = classify_comment(user_input)\n\u001b[32m    102\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🚀 Prediction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[43mlime_explain\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 77\u001b[39m, in \u001b[36mlime_explain\u001b[39m\u001b[34m(comment)\u001b[39m\n\u001b[32m     74\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.array([classify_comment(text, lime_mode=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m cleaned_texts])\n\u001b[32m     76\u001b[39m cleaned_comment = clean_text(comment)  \u001b[38;5;66;03m# Preprocess input text\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m exp = \u001b[43mexplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexplain_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcleaned_comment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_predict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# Extract words and scores\u001b[39;00m\n\u001b[32m     80\u001b[39m words, scores = \u001b[38;5;28mzip\u001b[39m(*exp.as_list())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\lime\\lime_text.py:413\u001b[39m, in \u001b[36mLimeTextExplainer.explain_instance\u001b[39m\u001b[34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[39m\n\u001b[32m    406\u001b[39m indexed_string = (IndexedCharacters(\n\u001b[32m    407\u001b[39m     text_instance, bow=\u001b[38;5;28mself\u001b[39m.bow, mask_string=\u001b[38;5;28mself\u001b[39m.mask_string)\n\u001b[32m    408\u001b[39m                   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.char_level \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m    409\u001b[39m                   IndexedString(text_instance, bow=\u001b[38;5;28mself\u001b[39m.bow,\n\u001b[32m    410\u001b[39m                                 split_expression=\u001b[38;5;28mself\u001b[39m.split_expression,\n\u001b[32m    411\u001b[39m                                 mask_string=\u001b[38;5;28mself\u001b[39m.mask_string))\n\u001b[32m    412\u001b[39m domain_mapper = TextDomainMapper(indexed_string)\n\u001b[32m--> \u001b[39m\u001b[32m413\u001b[39m data, yss, distances = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__data_labels_distances\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexed_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdistance_metric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistance_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.class_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    417\u001b[39m     \u001b[38;5;28mself\u001b[39m.class_names = [\u001b[38;5;28mstr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(yss[\u001b[32m0\u001b[39m].shape[\u001b[32m0\u001b[39m])]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\lime\\lime_text.py:482\u001b[39m, in \u001b[36mLimeTextExplainer.__data_labels_distances\u001b[39m\u001b[34m(self, indexed_string, classifier_fn, num_samples, distance_metric)\u001b[39m\n\u001b[32m    480\u001b[39m     data[i, inactive] = \u001b[32m0\u001b[39m\n\u001b[32m    481\u001b[39m     inverse_data.append(indexed_string.inverse_removing(inactive))\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m labels = \u001b[43mclassifier_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minverse_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    483\u001b[39m distances = distance_fn(sp.sparse.csr_matrix(data))\n\u001b[32m    484\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data, labels, distances\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 74\u001b[39m, in \u001b[36mlime_explain.<locals>.model_predict\u001b[39m\u001b[34m(texts)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns probability distributions for LIME while ensuring '*' does not split words.\"\"\"\u001b[39;00m\n\u001b[32m     73\u001b[39m cleaned_texts = [clean_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]  \u001b[38;5;66;03m# Preprocess texts\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m74\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m np.array([\u001b[43mclassify_comment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlime_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m cleaned_texts])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mclassify_comment\u001b[39m\u001b[34m(comment, lime_mode)\u001b[39m\n\u001b[32m     39\u001b[39m inputs = tokenizer(comment, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, truncation=\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m.logits\n\u001b[32m     42\u001b[39m probs = torch.nn.functional.softmax(outputs, dim=\u001b[32m1\u001b[39m).cpu().numpy()[\u001b[32m0\u001b[39m]\n\u001b[32m     44\u001b[39m model_probs[model_name] = probs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:1000\u001b[39m, in \u001b[36mDebertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    992\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    993\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    994\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    995\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    996\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    997\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    998\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1000\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1008\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1009\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1011\u001b[39m encoder_layer = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1012\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(encoder_layer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:708\u001b[39m, in \u001b[36mDebertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    698\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m    700\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    701\u001b[39m     input_ids=input_ids,\n\u001b[32m    702\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    705\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    706\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:584\u001b[39m, in \u001b[36mDebertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    582\u001b[39m rel_embeddings = \u001b[38;5;28mself\u001b[39m.get_rel_embedding()\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m     hidden_states, att_m = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    593\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    594\u001b[39m         all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:512\u001b[39m, in \u001b[36mDebertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    503\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    504\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    505\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    510\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    511\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m     attention_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m    521\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:447\u001b[39m, in \u001b[36mDebertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    439\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    440\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    445\u001b[39m     rel_embeddings=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    446\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m     self_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    456\u001b[39m         query_states = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elvin\\miniconda3\\Lib\\site-packages\\transformers\\models\\deberta\\modeling_deberta.py:277\u001b[39m, in \u001b[36mDisentangledSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    274\u001b[39m     attention_scores = \u001b[38;5;28mself\u001b[39m.head_logits_proj(attention_scores.permute(\u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m)).permute(\u001b[32m0\u001b[39m, \u001b[32m3\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    276\u001b[39m attention_mask = attention_mask.bool()\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m attention_scores = \u001b[43mattention_scores\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[43m~\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmin\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;66;03m# bsz x height x length x dimension\u001b[39;00m\n\u001b[32m    279\u001b[39m attention_probs = nn.functional.softmax(attention_scores, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "# Model Paths (Update these paths accordingly)\n",
    "MODEL_PATHS = {\n",
    "    \"electra\": \"D:/Project Phase 1/Fine_Tuned Models/best_electra_model (1)\",\n",
    "    \"deberta\": \"D:/Project Phase 1/Fine_Tuned Models/best_deberta\",\n",
    "    \"roberta\": \"D:/Project Phase 1/Fine_Tuned Models/updated_twitter_roberta\",\n",
    "    \"hatebert\": \"D:/Project Phase 1/Fine_Tuned Models/best_hatebert\",\n",
    "}\n",
    "\n",
    "# Load Models and Tokenizers\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    models[name] = AutoModelForSequenceClassification.from_pretrained(path).to(device).eval()\n",
    "\n",
    "# Label Mapping\n",
    "label_mapping = {0: \"Severely Toxic\", 1: \"Moderately Toxic\", 2: \"Non-Toxic\"}\n",
    "\n",
    "def classify_comment(comment, lime_mode=False):\n",
    "    \"\"\"Classifies a given comment and returns either probabilities or full classification.\"\"\"\n",
    "    ensemble_preds = np.zeros(3)  # Assuming 3 classes: 0, 1, 2\n",
    "    model_probs = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        # if model_name == \"roberta\":\n",
    "        #     probs = np.array([1.0, 0.0, 0.0])  # Force RoBERTa to always classify as \"Severely Toxic\"\n",
    "        # else:\n",
    "        tokenizer = tokenizers[model_name]\n",
    "        model = models[model_name]\n",
    "        inputs = tokenizer(comment, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs).logits\n",
    "        probs = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()[0]\n",
    "\n",
    "        model_probs[model_name] = probs\n",
    "        ensemble_preds += probs / len(models)  # Uniform weighting\n",
    "    \n",
    "    if lime_mode:\n",
    "        return ensemble_preds  # Return probabilities for LIME\n",
    "\n",
    "    predicted_label = np.argmax(ensemble_preds)\n",
    "    \n",
    "    print(\"/n🔍 Model Probabilities:\")\n",
    "    for model_name, probs in model_probs.items():\n",
    "        print(f\"{model_name}: Non-Toxic: {probs[2]:.4f}, Moderately Toxic: {probs[1]:.4f}, Severely Toxic: {probs[0]:.4f}\")\n",
    "    \n",
    "    print(\"/n🎯 Ensemble Probabilities:\")\n",
    "    print(f\"Non-Toxic: {ensemble_preds[2]:.4f}, Moderately Toxic: {ensemble_preds[1]:.4f}, Severely Toxic: {ensemble_preds[0]:.4f}\")\n",
    "\n",
    "    return label_mapping[predicted_label], ensemble_preds\n",
    "\n",
    "# LIME Explanation Setup\n",
    "explainer = LimeTextExplainer(class_names=[\"Severely Toxic\", \"Moderately Toxic\", \"Non-Toxic\"])\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes special characters like '*' without breaking words.\"\"\"\n",
    "    return re.sub(r'/*', '', text)  # Removes all asterisks but keeps word structure intact\n",
    "\n",
    "def lime_explain(comment):\n",
    "    \"\"\"Generates a LIME explanation and saves a PNG plot of word importance.\"\"\"\n",
    "\n",
    "    def model_predict(texts):\n",
    "        \"\"\"Returns probability distributions for LIME while ensuring '*' does not split words.\"\"\"\n",
    "        cleaned_texts = [clean_text(text) for text in texts]  # Preprocess texts\n",
    "        return np.array([classify_comment(text, lime_mode=True) for text in cleaned_texts])\n",
    "\n",
    "    cleaned_comment = clean_text(comment)  # Preprocess input text\n",
    "    exp = explainer.explain_instance(cleaned_comment, model_predict, num_features=10)\n",
    "\n",
    "    # Extract words and scores\n",
    "    words, scores = zip(*exp.as_list())\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.barh(words, scores, color=\"blue\")\n",
    "    plt.xlabel(\"Toxicity Score\")\n",
    "    plt.ylabel(\"Word\")\n",
    "    plt.title(\"LIME Word Importance for Toxicity\")\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    # Save as PNG\n",
    "    plt.savefig(\"lime_explanation.png\")\n",
    "    plt.show()\n",
    "\n",
    "# User Input Loop\n",
    "print(\"/n🔥 Abusive Comment Detection System 🔥/nEnter text to classify (type 'exit' to quit):\")\n",
    "while True:\n",
    "    user_input = input(\"/nEnter comment: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Exiting...👋\")\n",
    "        break\n",
    "    result, _ = classify_comment(user_input)\n",
    "    print(f\"🚀 Prediction: {result}\")\n",
    "    lime_explain(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6914346,
     "sourceId": 11092041,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6915349,
     "sourceId": 11093447,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6915762,
     "sourceId": 11094094,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 272253,
     "modelInstanceId": 250762,
     "sourceId": 292719,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 272267,
     "modelInstanceId": 250777,
     "sourceId": 292740,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
