{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10997352,"sourceType":"datasetVersion","datasetId":6845810},{"sourceId":11039363,"sourceType":"datasetVersion","datasetId":6876153}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer\n\n# Load preprocessed train and test datasets\ntrain_df = pd.read_csv(\"/kaggle/input/opt-dataset/final optimised dataset/train_dataset (1).csv\")\ntest_df = pd.read_csv(\"/kaggle/input/opt-dataset/final optimised dataset/test.csv\")\ntest_labels_df = pd.read_csv(\"/kaggle/input/opt-dataset/final optimised dataset/test_labels.csv\")\n\n# Merge test set with labels\ntest_df = test_df.merge(test_labels_df, on=\"id\")\n\n# Identify label columns (assuming they start from 2nd column in test_labels_df)\nlabel_columns = list(test_labels_df.columns[1:])  # Excluding 'id' column\n\n# Convert pandas to Hugging Face dataset\ntrain_ds = Dataset.from_pandas(train_df)\ntest_ds = Dataset.from_pandas(test_df)\n\n# Load Deberta tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n\n# First, check if the column exists and what type of data it contains\nprint(test_ds.column_names)  # Check if \"comment_text\" exists\nprint(type(test_ds[\"comment_text\"][0]) if \"comment_text\" in test_ds.column_names else \"Column not found\")\n\n# Modified tokenization function with error handling\ndef tokenize_function(examples):\n    # Filter out None/NaN values and ensure all inputs are strings\n    texts = []\n    for text in examples[\"comment_text\"]:\n        if text is None:\n            texts.append(\"\")  # Replace None with empty string\n        elif not isinstance(text, str):\n            texts.append(str(text))  # Convert non-strings to strings\n        else:\n            texts.append(text)\n    \n    return tokenizer(\n        texts,\n        truncation=True,\n        padding=\"max_length\",\n        max_length=128,\n        return_tensors=\"np\"  # Use numpy arrays instead of PyTorch tensors for now\n    )\n\n# Try processing in smaller batches\ntrain_ds = train_ds.map(tokenize_function, batched=True, batch_size=64)\ntest_ds = test_ds.map(tokenize_function, batched=True, batch_size=64)\n\n# Ensure labels are structured as a list of floats\ndef format_labels(example):\n    example[\"labels\"] = [float(example[col]) for col in label_columns]  # Convert labels to float\n    return example\n\ntrain_ds = train_ds.map(format_labels)\ntest_ds = test_ds.map(format_labels)\n\n# Remove unnecessary columns (text, id, and individual label columns after structuring)\ncolumns_to_remove = [\"comment_text\", \"id\"] + label_columns\ntrain_ds = train_ds.remove_columns(columns_to_remove)\ntest_ds = test_ds.remove_columns(columns_to_remove)\n\n# Ensure dataset format remains list of floats\ndef ensure_list_format(example):\n    example[\"labels\"] = [float(x) for x in example[\"labels\"]]  # Ensure list of floats\n    return example\n\ntrain_ds = train_ds.map(ensure_list_format)\ntest_ds = test_ds.map(ensure_list_format)\n\nprint(\"âœ… Deberta Dataset loaded, tokenized & formatted successfully!\")\n\n# Save datasets (Optional: Avoid re-running preprocessing)\ntrain_ds.save_to_disk(\"/kaggle/working/train_ds_deberta\")\ntest_ds.save_to_disk(\"/kaggle/working/test_ds_deberta\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:01:54.393864Z","iopub.execute_input":"2025-03-15T15:01:54.394173Z","iopub.status.idle":"2025-03-15T15:04:22.413858Z","shell.execute_reply.started":"2025-03-15T15:01:54.394146Z","shell.execute_reply":"2025-03-15T15:04:22.412816Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/579 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f9146c67d74da4be974494786b3c97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spm.model:   0%|          | 0.00/2.46M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f77358e6e28040fdb4974e302c8939ea"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"['id', 'comment_text', 'severely_toxic', 'moderately_toxic', 'non_toxic']\n<class 'str'>\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0a19be341a84066a7fcf0e39e648661"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"595023c6733f4e3e95e0e87d2420f419"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d938b54c27472cbd855161f304826d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55633bd0d0124e42a6c84696f0f28178"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a643f4e60eaf462b9f1b0e0cf5c82870"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bec097c153544849bb2e0c42827a6d0"}},"metadata":{}},{"name":"stdout","text":"âœ… Deberta Dataset loaded, tokenized & formatted successfully!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/298513 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84e9d07419f444d3871f74041e0f9d55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/127936 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3b24cf03dde403fbd66ce7a93735677"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback, AdamW\nimport shutil\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nimport numpy as np\n\n# Load DeBERTa Base model with sigmoid outputs for multi-label classification\ndeberta_model = AutoModelForSequenceClassification.from_pretrained(\n    \"microsoft/deberta-v3-base\", \n    num_labels=3,  # Matches one-hot encoding format\n    problem_type=\"multi_label_classification\"\n)\n\n# Increase dropout for better regularization\ndeberta_model.config.hidden_dropout_prob = 0.3\ndeberta_model.config.attention_probs_dropout_prob = 0.3\n\n# Freeze Transformer Layers, Train Only Classifier\nfor param in deberta_model.deberta.parameters():\n    param.requires_grad = False  \nfor param in deberta_model.classifier.parameters():\n    param.requires_grad = True  \n\n# Hybrid Loss (Focal + BCE With Logits)\nclass HybridLoss(nn.Module):\n    def __init__(self, alpha=0.75, gamma=2.0, ce_weight=0.5):\n        super(HybridLoss, self).__init__()\n        self.focal = FocalLoss(alpha, gamma)\n        self.bce = nn.BCEWithLogitsLoss()\n        self.ce_weight = ce_weight\n        \n    def forward(self, inputs, targets):\n        return self.ce_weight * self.bce(inputs, targets.float()) + (1 - self.ce_weight) * self.focal(inputs, targets)\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=0.5, gamma=2.0):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n        \n    def forward(self, inputs, targets):\n        BCE_loss = self.bce(inputs, targets.float())\n        probs = torch.sigmoid(inputs)\n        pt = torch.where(targets.bool(), probs, 1-probs)\n        alpha_factor = torch.where(targets.bool(), self.alpha, 1 - self.alpha)\n        focal_weight = alpha_factor * (1 - pt).pow(self.gamma)\n        return (focal_weight * BCE_loss).mean()\n\n# Training arguments with auto-save best model - using epochs instead of steps\ntraining_args = TrainingArguments(\n    output_dir=\"./deberta_model\",\n    evaluation_strategy=\"epoch\",      # Evaluation once per epoch\n    save_strategy=\"epoch\",            # Save once per epoch\n    save_total_limit=1,\n    logging_strategy=\"epoch\",         # Log once per epoch\n    learning_rate=8e-6,               # Slightly higher LR for base model compared to large\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.1,\n    per_device_train_batch_size=8,    # Can use larger batch size with smaller model\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.1,\n    report_to=\"none\",\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_auc\",\n    greater_is_better=True,\n    max_grad_norm=1.0,\n    label_smoothing_factor=0.05,\n    fp16=True,\n    gradient_accumulation_steps=1,    # Can reduce or eliminate with smaller model\n    seed=42,\n    data_seed=42,\n)\n\n# Compute Metrics Function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    sigmoid_logits = torch.sigmoid(torch.tensor(logits)).numpy()\n    predictions = (sigmoid_logits > 0.5).astype(np.int8)\n    \n    accuracy = accuracy_score(labels, predictions)\n    \n    try:\n        f1_micro = f1_score(labels, predictions, average='micro', zero_division=0)\n        f1_macro = f1_score(labels, predictions, average='macro', zero_division=0)\n        auc_scores = [roc_auc_score(labels[:, i], sigmoid_logits[:, i]) for i in range(labels.shape[1]) if len(np.unique(labels[:, i])) > 1]\n        mean_auc = np.mean(auc_scores) if auc_scores else 0.0\n    except Exception as e:\n        print(f\"Error in metric calculation: {e}\")\n        f1_micro, f1_macro, mean_auc = 0, 0, 0\n    \n    return {\"accuracy\": accuracy, \"f1_micro\": f1_micro, \"f1_macro\": f1_macro, \"auc\": mean_auc}\n\n# Custom Trainer with Hybrid Loss\nclass HybridLossTrainer(Trainer):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.loss_fn = HybridLoss(alpha=0.5, gamma=2.0, ce_weight=0.5)\n        \n    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):  \n        labels = inputs[\"labels\"].float()  # Keep labels in one-hot format\n        outputs = model(**inputs)\n        logits = outputs.logits\n        loss = self.loss_fn(logits, labels)\n        return (loss, outputs) if return_outputs else loss\n\n# Define Trainer\ntrainer = HybridLossTrainer(\n    model=deberta_model,\n    args=training_args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=0.001)]\n)\n\n# Step 1: Train Only Classifier\nprint(\"ðŸš€ Training classifier only...\")\ntrainer.train()\n\n# Step 2: Unfreeze last 4 layers EXACTLY\nprint(\"ðŸ”“ Unfreezing last 4 layers for further fine-tuning...\")\nfor layer in deberta_model.deberta.encoder.layer[-4:]:\n    for param in layer.parameters():\n        param.requires_grad = True\n\n# Apply Layer-wise Learning Rate\noptimizer = AdamW([\n    {\"params\": [p for n, p in deberta_model.deberta.named_parameters() if \"encoder.layer\" not in n or \n               (int(n.split(\"encoder.layer.\")[1].split(\".\")[0]) < len(deberta_model.deberta.encoder.layer) - 4)], \"lr\": 2e-6},\n    {\"params\": [p for n, p in deberta_model.deberta.named_parameters() if \"encoder.layer\" in n and \n               (int(n.split(\"encoder.layer.\")[1].split(\".\")[0]) >= len(deberta_model.deberta.encoder.layer) - 4)], \"lr\": 5e-6},\n    {\"params\": deberta_model.classifier.parameters(), \"lr\": 8e-6},\n], weight_decay=0.1)\n\ntrainer.args.num_train_epochs = 2  \ntrainer.optimizer = optimizer  \n\n# Step 2: Fine-tune last 4 layers\nprint(\"ðŸš€ Fine-tuning last 4 layers...\")\ntrainer.train()\n\n# Save best model\nbest_model_path = \"./best_deberta_base\"\ntrainer.save_model(best_model_path)\ntokenizer.save_pretrained(best_model_path)\nshutil.make_archive(best_model_path, 'zip', best_model_path)\nprint(\"âœ… Best Model Saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T15:07:54.784643Z","iopub.execute_input":"2025-03-15T15:07:54.784969Z","iopub.status.idle":"2025-03-15T21:39:28.714161Z","shell.execute_reply.started":"2025-03-15T15:07:54.784943Z","shell.execute_reply":"2025-03-15T21:39:28.713341Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/371M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"383a98cef0944ebda556a39a96e7ed7e"}},"metadata":{}},{"name":"stderr","text":"Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-6-1fd5eaa0ecdf>:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `HybridLossTrainer.__init__`. Use `processing_class` instead.\n  super().__init__(*args, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"ðŸš€ Training classifier only...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='37316' max='37316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [37316/37316 2:54:35, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n      <th>Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.273800</td>\n      <td>0.211697</td>\n      <td>0.620717</td>\n      <td>0.723639</td>\n      <td>0.707308</td>\n      <td>0.906977</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.217300</td>\n      <td>0.200083</td>\n      <td>0.652983</td>\n      <td>0.750856</td>\n      <td>0.737145</td>\n      <td>0.916075</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"ðŸ”“ Unfreezing last 4 layers for further fine-tuning...\nðŸš€ Fine-tuning last 4 layers...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='37316' max='37316' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [37316/37316 3:34:33, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1 Micro</th>\n      <th>F1 Macro</th>\n      <th>Auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.120900</td>\n      <td>0.137014</td>\n      <td>0.969883</td>\n      <td>0.971527</td>\n      <td>0.971638</td>\n      <td>0.997113</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.129100</td>\n      <td>0.134712</td>\n      <td>0.973455</td>\n      <td>0.974453</td>\n      <td>0.974503</td>\n      <td>0.997372</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Best Model Saved!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}